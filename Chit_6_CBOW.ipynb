{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityakangune/IT-LP-4-Codes-/blob/main/Chit_6_CBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chit 6**\n",
        "\n",
        "### *Name:Aditya Kangune*\n",
        "### *Roll number: 43321*\n",
        "### *Batch: P11*"
      ],
      "metadata": {
        "id": "SOZWUJDdBV9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Problem Statement:*\n",
        "\n",
        "    Implement the Continuous Bag of Words (CBOW) Model for the given (textual document 1) using the below steps:\n",
        "    a. Data preparation\n",
        "    b. Generate training data\n",
        "    c. Train model\n",
        "    d. Output\n",
        "\n"
      ],
      "metadata": {
        "id": "T9f0RSeUJLP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Save the following text in a corona.txt file before"
      ],
      "metadata": {
        "id": "GeoUI8RBlLeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19. \n",
        "\n",
        "Further, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission. \n",
        "\n",
        "The reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.  \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "t4mNSp1FlTs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "pJO70ku3Ragn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V51q50EbF-T9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=open('/content/corona.txt','r')\n",
        "corona_data = [text for text in data if text.count(' ') >= 2]\n",
        "vectorize = Tokenizer()"
      ],
      "metadata": {
        "id": "HslLN21ffhSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit data to tokenizer\n"
      ],
      "metadata": {
        "id": "iJiL6QSRfzO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBUwYdBJElVz"
      },
      "outputs": [],
      "source": [
        "vectorize.fit_on_texts(corona_data)\n",
        "corona_data = vectorize.texts_to_sequences(corona_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find total no of words and total no of sentences.\n",
        "total_vocab = sum(len(s) for s in corona_data)\n",
        "word_count = len(vectorize.word_index) + 1\n",
        "window_size = 2"
      ],
      "metadata": {
        "id": "8bCcicjif4i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the pairs of Context words and target words\n",
        "def cbow_model(data, window_size, total_vocab):\n",
        "    total_length = window_size*2\n",
        "    for text in data:\n",
        "        text_len = len(text)\n",
        "        for idx, word in enumerate(text):\n",
        "            context_word = []\n",
        "            target   = []            \n",
        "            begin = idx - window_size\n",
        "            end = idx + window_size + 1\n",
        "            context_word.append([text[i] for i in range(begin, end) if 0 <= i < text_len and i != idx])\n",
        "            target.append(word)\n",
        "            contextual = sequence.pad_sequences(context_word, total_length=total_length)\n",
        "            final_target = np_utils.to_categorical(target, total_vocab)\n",
        "            yield(contextual, final_target)"
      ],
      "metadata": {
        "id": "akOXPLfBgE8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Neural Network model with following parameters :\n",
        "\n",
        "    Model type : sequential\n",
        "    \n",
        "    Layers : Dense , Lambda , embedding. Compile\n",
        "\n",
        "    Options : (loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "EvCwshtogInC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\n",
        "model.add(Dense(total_vocab, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "for i in range(10):\n",
        "    cost = 0\n",
        "    for x, y in cbow_model(data, window_size, total_vocab):\n",
        "        cost += model.train_on_batch(contextual, final_target)\n",
        "    print(i, cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AJOQ_RBgH0R",
        "outputId": "ed406108-abc5-44fb-c4df-f08eeba195c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n",
            "1 0\n",
            "2 0\n",
            "3 0\n",
            "4 0\n",
            "5 0\n",
            "6 0\n",
            "7 0\n",
            "8 0\n",
            "9 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector file of some word for testing\n",
        "dimensions=100\n",
        "vect_file = open('/content/vectors.txt' ,'w')\n",
        "vect_file.write('{} {}\\n'.format(total_vocab,dimensions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub-YzNuvgHwr",
        "outputId": "992877ba-e94f-4620-c8df-b2bb9f28c308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assign weights to your trained model\n",
        "weights = model.get_weights()[0]\n",
        "for text, i in vectorize.word_index.items():\n",
        "    final_vec = ' '.join(map(str, list(weights[i, :])))\n",
        "    vect_file.write('{} {}\\n'.format(text, final_vec))\n",
        "vect_file.close()"
      ],
      "metadata": {
        "id": "4U-4kh1Ygb_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the vectors created in Gemsim\n",
        "cbow_output = gensim.models.KeyedVectors.load_word2vec_format('/content/vectors.txt', binary = False, limit=100)"
      ],
      "metadata": {
        "id": "Pte5kYhAgb8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose the word to get similar type of words\n",
        "cbow_output.most_similar(positive=['virus'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keSXc08Lgb5f",
        "outputId": "fa082128-56c1-4c6c-defb-f1ba73360e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('19', 0.2752428650856018),\n",
              " ('24', 0.2098691165447235),\n",
              " ('of', 0.1796838790178299),\n",
              " ('between', 0.16798871755599976),\n",
              " ('in', 0.14854368567466736),\n",
              " ('period', 0.14530067145824432),\n",
              " ('symptomatic', 0.14341437816619873),\n",
              " ('further', 0.12186173349618912),\n",
              " ('influenza', 0.11704269051551819),\n",
              " ('appearance', 0.1145254373550415)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FzwNBXq-gHuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9d0NjxTlgHrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjlBj2J1g-oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "egcB5gd3g-kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQ4T2GJvg-hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0MnRl1jg-e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k7Io9iYDg-b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JxgK4k2Ug-ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MIy_gbPPg-XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2XBtQjQg-Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTa-cL7ug-SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "celNk9LmEvm8"
      },
      "outputs": [],
      "source": [
        "# #tokenization\n",
        "# tokenizer = text.Tokenizer()\n",
        "# tokenizer.fit_on_texts(dl_data)\n",
        "# word2id = tokenizer.word_index\n",
        "\n",
        "# word2id['PAD'] = 0\n",
        "# id2word = {v:k for k, v in word2id.items()}\n",
        "# wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
        "\n",
        "# vocab_size = len(word2id)\n",
        "# embed_size = 100\n",
        "# window_size = 2 \n",
        "\n",
        "# print('Vocabulary Size:', vocab_size)\n",
        "# print('Vocabulary Sample:', list(word2id.items())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAxNYDanInQC"
      },
      "outputs": [],
      "source": [
        "# #generating (context word, target/label word) pairs\n",
        "# def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "#     context_length = window_size*2\n",
        "#     for words in corpus:\n",
        "#         sentence_length = len(words)\n",
        "#         for index, word in enumerate(words):\n",
        "#             context_words = []\n",
        "#             label_word   = []            \n",
        "#             start = index - window_size\n",
        "#             end = index + window_size + 1\n",
        "            \n",
        "#             context_words.append([words[i] \n",
        "#                                  for i in range(start, end) \n",
        "#                                  if 0 <= i < sentence_length \n",
        "#                                  and i != index])\n",
        "#             label_word.append(word)\n",
        "\n",
        "#             x = pad_sequences(context_words, maxlen=context_length)\n",
        "#             y = np_utils.to_categorical(label_word, vocab_size)\n",
        "#             yield (x, y)\n",
        "            \n",
        "# i = 0\n",
        "# for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "#     if 0 not in x[0]:\n",
        "#         # print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "    \n",
        "#         if i == 10:\n",
        "#             break\n",
        "#         i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb5dNmoZKZBv"
      },
      "outputs": [],
      "source": [
        "# #model building\n",
        "# import keras.backend as K\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "# cbow = Sequential()\n",
        "# cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
        "# cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
        "# cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "# cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "# print(cbow.summary())\n",
        "\n",
        "# # from IPython.display import SVG\n",
        "# # from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "# # SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs12C3MDK1q4"
      },
      "outputs": [],
      "source": [
        "# for epoch in range(1, 6):\n",
        "#     loss = 0.\n",
        "#     i = 0\n",
        "#     for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "#         i += 1\n",
        "#         loss += cbow.train_on_batch(x, y)\n",
        "#         if i % 100000 == 0:\n",
        "#             print('Processed {} (context, word) pairs'.format(i))\n",
        "\n",
        "#     print('Epoch:', epoch, '\\tLoss:', loss)\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ3_QGKVK6Tj"
      },
      "outputs": [],
      "source": [
        "# weights = cbow.get_weights()[0]\n",
        "# weights = weights[1:]\n",
        "# print(weights.shape)\n",
        "\n",
        "# pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFs2IAn_LAYS"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# distance_matrix = euclidean_distances(weights)\n",
        "# print(distance_matrix.shape)\n",
        "\n",
        "# similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "#                    for search_term in ['disease']}\n",
        "\n",
        "# similar_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GePKWe9AR12s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}